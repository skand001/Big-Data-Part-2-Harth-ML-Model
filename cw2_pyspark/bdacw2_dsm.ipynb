{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOzaeiexLlMH"
   },
   "source": [
    "**Big Data Analysis Coursework 2**\n",
    "\n",
    "**Harth Motion Sensor Data**\n",
    "\n",
    "**Author: Sandor Kanda**\n",
    "\n",
    "**Source: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkNsy1BWxszv",
    "outputId": "0f15c66b-1af9-44ec-8e74-a7fb0efa0162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2024-05-07 20:24:26.813276\n",
      "pyspark is already installed.\n",
      "Time Spent: 0 minutes, 0 seconds\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import datetime\n",
    "\n",
    "# Function to check if a module is available\n",
    "def is_module_available(module_name):\n",
    "    return importlib.util.find_spec(module_name) is not None\n",
    "\n",
    "# Start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "# Check if 'pyspark' is installed\n",
    "if not is_module_available('pyspark'):\n",
    "    !pip install pyspark\n",
    "else:\n",
    "    print(\"pyspark is already installed.\")\n",
    "\n",
    "# End and Duration Calculation\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "minutes, seconds = divmod(duration.total_seconds(), 60)\n",
    "print(f\"Time Spent: {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xxVDfQiLL01O"
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, PCA\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import col, count, mean, stddev, min, max, when\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier, LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oN0tSReKNNO7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2024-05-07 20:24:28.269813\n",
      "Time Spent: 0 minutes, 32 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create a new SparkSession (which will also create a new SparkContext)\n",
    "\n",
    "# Start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"harth\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access the SparkContext from the new SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# End and Duration Calculation\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "minutes, seconds = divmod(duration.total_seconds(), 60)\n",
    "print(f\"Time Spent: {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VMFjdYdBNb7i",
    "outputId": "15108978-e57b-47a3-8814-c06ac6f6e077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2024-05-07 20:25:01.346754\n"
     ]
    }
   ],
   "source": [
    "# Start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "# Path to the folder containing CSV files on HDFS\n",
    "folder_path = 'hdfs:///user/skand001/harth_2/'\n",
    "# folder_path = './harth_2/'\n",
    "\n",
    "# Read all CSV files from the directory in HDFS using wildcard\n",
    "combined_df = spark.read.csv(folder_path + '*.csv', header=True, inferSchema=True)\n",
    "\n",
    "# End and Duration Calculations\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "minutes, seconds = divmod(duration.total_seconds(), 60)\n",
    "print(f\"Time Spent: {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yF2NHyWEWmvL"
   },
   "outputs": [],
   "source": [
    "sampled_df = combined_df.sample(withReplacement=False, fraction=0.2, seed=42)\n",
    "spark_data = combined_df\n",
    "spark_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBYobvpEXhxO",
    "outputId": "96312dc7-8363-4fce-8a17-34e84dc033f2"
   },
   "outputs": [],
   "source": [
    "# Start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "# Univariate Analysis\n",
    "spark_data.describe().show()\n",
    "\n",
    "# Bivariate Analysis\n",
    "input_cols = [\"back_x\", \"back_y\", \"back_z\", \"thigh_x\", \"thigh_y\", \"thigh_z\"]\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "feature_df = assembler.transform(spark_data)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = Correlation.corr(feature_df, \"features\").head()[0]\n",
    "corr_matrix_array = corr_matrix.toArray()\n",
    "corr_df = spark.createDataFrame(\n",
    "    [(col,) + tuple(float(x) for x in corr_matrix_array[i]) for i, col in enumerate(input_cols)],\n",
    "    [\"Column\"] + input_cols\n",
    ")\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "corr_df.show(truncate=False)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(feature_df)\n",
    "pca_df = pca_model.transform(feature_df)\n",
    "pca_df.show()\n",
    "\n",
    "# End and Duration Calculation\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "minutes, seconds = divmod(duration.total_seconds(), 60)\n",
    "print(f\"Time Spent: {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "p4bCKzSoxIkb",
    "outputId": "de112746-39eb-4fb8-cf96-93e635492416"
   },
   "outputs": [],
   "source": [
    "# Convert the sampled Spark DataFrame to a Pandas DataFrame for visualization\n",
    "# Start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "sampled_data = sampled_df.toPandas()\n",
    "# A scatter matrix of the sampled data\n",
    "sns.pairplot(sampled_data, vars=[\"back_x\", \"back_y\", \"back_z\", \"thigh_x\", \"thigh_y\", \"thigh_z\"], hue=\"label\")\n",
    "plt.show()\n",
    "\n",
    "# End and Duration Calculation\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "minutes, seconds = divmod(duration.total_seconds(), 60)\n",
    "print(f\"Time Spent: {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "aSrlxxqxxRP5",
    "outputId": "3c37b04f-e740-44db-e115-5f570b0bd52d"
   },
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "corr = sampled_data[[\"back_x\", \"back_y\", \"back_z\", \"thigh_x\", \"thigh_y\", \"thigh_z\"]].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input columns and the assembler\n",
    "input_cols = ['back_x', 'back_y', 'back_z', 'thigh_x', 'thigh_y', 'thigh_z']\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StringIndexer to create indexed labels\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
    "indexed_data = indexer.fit(assembled_data).transform(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifiers\n",
    "\n",
    "# Start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "rf_classifier = RandomForestClassifier(labelCol=\"indexedLabel\", \n",
    "                                       featuresCol=\"features\", \n",
    "                                       numTrees=10)\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(labelCol=\"indexedLabel\", \n",
    "                                       featuresCol=\"features\")\n",
    "\n",
    "lr_classifier = LogisticRegression(labelCol=\"label\", \n",
    "                                   featuresCol=\"features\")\n",
    "\n",
    "gbt_classifier = GBTClassifier(labelCol=\"label\", \n",
    "                               featuresCol=\"features\", \n",
    "                               maxIter=10)\n",
    "\n",
    "# End and Duration Calculation\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "minutes, seconds = divmod(duration.total_seconds(), 60)\n",
    "print(f\"Time Spent: {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "train_data, val_test_data = indexed_data.randomSplit([0.7, 0.3], seed=42)\n",
    "val_data, test_data = val_test_data.randomSplit([0.5, 0.5], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the distinct number of labels\n",
    "distinct_label_count = train_data.select(\"indexedLabel\").distinct().count()\n",
    "print(\"Total number of distinct labels:\", distinct_label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = train_data.select(\"indexedLabel\").distinct().collect()\n",
    "print(\"All distinct labels:\", [row.indexedLabel for row in all_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupBy(\"indexedLabel\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifiers\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# Start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "rf_classifier = RandomForestClassifier(labelCol=\"indexedLabel\", \n",
    "                                       featuresCol=\"features\", \n",
    "                                       numTrees=10)\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(labelCol=\"indexedLabel\", \n",
    "                                       featuresCol=\"features\")\n",
    "\n",
    "lr_classifier = LogisticRegression(labelCol=\"indexedLabel\", \n",
    "                                   featuresCol=\"features\")\n",
    "\n",
    "layers = [len(input_cols), 128, 64, 11]\n",
    "mlp = MultilayerPerceptronClassifier(layers=layers, \n",
    "                                     labelCol=\"indexedLabel\", \n",
    "                                     featuresCol=\"features\")\n",
    "\n",
    "# End and Duration Calculation\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "minutes, seconds = divmod(duration.total_seconds(), 60)\n",
    "print(f\"Time Spent: {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines\n",
    "pipeline_rf = Pipeline(stages=[rf_classifier])\n",
    "pipeline_dt = Pipeline(stages=[dt_classifier])\n",
    "pipeline_lr = Pipeline(stages=[lr_classifier])\n",
    "pipeline_mlp = Pipeline(stages=[mlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "# Fit the models\n",
    "rf_model = pipeline_rf.fit(train_data)\n",
    "dt_model = pipeline_dt.fit(train_data)\n",
    "lr_model = pipeline_lr.fit(train_data)\n",
    "mlp_model = pipeline_mlp.fit(train_data)\n",
    "\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(layers=layers, labelCol=\"indexedLabel\", featuresCol=\"features\")\n",
    "pipeline_mlp = Pipeline(stages=[mlp])\n",
    "mlp_model = pipeline_mlp.fit(train_data)\n",
    "\n",
    "# Cache the train data to speed up model fitting if still needed\n",
    "train_data.cache()\n",
    "\n",
    "# End and Duration Calculation\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "minutes, seconds = divmod(duration.total_seconds(), 60)\n",
    "print(f\"Time Spent: {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "from tqdm import tqdm\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "\n",
    "# Evaluate the models on the training set\n",
    "rf_train_metrics = {}\n",
    "dt_train_metrics = {}\n",
    "lr_train_metrics = {}\n",
    "mlp_train_metrics = {}\n",
    "\n",
    "print(\"Evaluating models on the training set...\")\n",
    "for metric, evaluator in tqdm([(\"Accuracy\", evaluator_accuracy), (\"F1-score\", evaluator_f1), (\"Precision\", evaluator_precision), (\"Recall\", evaluator_recall)]):\n",
    "    rf_train_metrics[metric] = evaluator.evaluate(rf_model.transform(train_data))\n",
    "    dt_train_metrics[metric] = evaluator.evaluate(dt_model.transform(train_data))\n",
    "    lr_train_metrics[metric] = evaluator.evaluate(lr_model.transform(train_data))\n",
    "    mlp_train_metrics[metric] = evaluator.evaluate(mlp_model.transform(train_data))\n",
    "\n",
    "# Evaluate the models on the validation set\n",
    "rf_val_metrics = {}\n",
    "dt_val_metrics = {}\n",
    "lr_val_metrics = {}\n",
    "mlp_val_metrics = {}\n",
    "\n",
    "print(\"Evaluating models on the validation set...\")\n",
    "for metric, evaluator in tqdm([(\"Accuracy\", evaluator_accuracy), (\"F1-score\", evaluator_f1), (\"Precision\", evaluator_precision), (\"Recall\", evaluator_recall)]):\n",
    "    rf_val_metrics[metric] = evaluator.evaluate(rf_model.transform(val_data))\n",
    "    dt_val_metrics[metric] = evaluator.evaluate(dt_model.transform(val_data))\n",
    "    lr_val_metrics[metric] = evaluator.evaluate(lr_model.transform(val_data))\n",
    "    mlp_val_metrics[metric] = evaluator.evaluate(mlp_model.transform(val_data))\n",
    "\n",
    "# End and Duration Calculation\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "minutes, seconds = divmod(duration.total_seconds(), 60)\n",
    "print(f\"Time Spent: {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "model_history = {\n",
    "    \"Random Forest\": {\n",
    "        \"Training\": rf_train_metrics,\n",
    "        \"Validation\": rf_val_metrics\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"Training\": dt_train_metrics,\n",
    "        \"Validation\": dt_val_metrics\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"Training\": lr_train_metrics,\n",
    "        \"Validation\": lr_val_metrics \n",
    "    },\n",
    "    \"Multi-Layer Perceptron\": {\n",
    "        \"Training\": mlp_train_metrics,\n",
    "        \"Validation\": mlp_val_metrics \n",
    "    }\n",
    "}  # Added the missing closing brace here\n",
    "\n",
    "for model_name, metrics in model_history.items():\n",
    "    print(f\"--- {model_name} Metrics ---\")\n",
    "    for metric_type, value in metrics.items():\n",
    "        if isinstance(value, dict):\n",
    "            # If the metric itself is a dictionary, iterate through its items\n",
    "            print(f\"{metric_type}:\")\n",
    "            for sub_key, sub_value in value.items():\n",
    "                print(f\"  {sub_key}: {sub_value}\")\n",
    "        else:\n",
    "            # If the metric is a scalar, just print it\n",
    "            print(f\"{metric_type}: {value}\")\n",
    "    print()  # Blank line for separation\n",
    "    \n",
    "# End and Duration Calculation\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "minutes, seconds = divmod(duration.total_seconds(), 60)\n",
    "print(f\"Time Spent: {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(model_history.keys())\n",
    "metrics = [\"Accuracy\", \"F1-score\", \"Precision\", \"Recall\"]\n",
    "\n",
    "# Create subplots for each metric\n",
    "fig, axs = plt.subplots(len(metrics), 1, figsize=(10, 20), sharex=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    train_values = [model_history[model][\"Training\"][metric] for model in models]\n",
    "    val_values = [model_history[model][\"Validation\"][metric] for model in models]\n",
    "    \n",
    "    axs[i].plot(models, train_values, marker='o', label=f\"Training {metric}\")\n",
    "    axs[i].plot(models, val_values, marker='o', label=f\"Validation {metric}\")\n",
    "    axs[i].set_ylabel(metric)\n",
    "    axs[i].legend()\n",
    "    axs[i].grid(True)\n",
    "\n",
    "axs[-1].set_xlabel(\"Model\")\n",
    "plt.suptitle(\"Model Performance Comparison\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
